{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03fdc7",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ed6c0",
   "metadata": {},
   "source": [
    "### 1.1 Preparación del entorno "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf379748",
   "metadata": {},
   "source": [
    "Aquí lo que hago es preaprar los imports principales y las variables de entorno. Ahora están comentadas porque como te dije en el correo estaba teniendo problemas con la versión de Cuda y Keras. Ahora lo he solucionado, pero basicamente lo que pasaba es que estaba usando una versión de Keras (Keras 3) la cual no es compatible con transfomers, al final lo soluciones activando  la variable de entorno TF_USE_LEGACY_KERAS e instalando en mi env de conda tf_keras para tener Keras 2. \n",
    "\n",
    "La variable TF_ENABLE_XLA la desactivé porque se supone que podía ser una causa de un problema de OOM que estaba teniendo durante el entrenamiento, pero al final resultó ser un problema con las versiones de las librerías.\n",
    "\n",
    "La variables de entorno que tienen que ver con CUDA era porque creía que mi entorno virtual de conda (el cual al final lo he tenido que meter en un WLS2 con ubuntu porque en windows estaba teniendo problemas de compatibilidad peores, estaba cogienod) estaba cogiendo la versión de CUDA que no era, porque tenía varias instaladas. Pero era más un fallo de configuración del entorno que eso.\n",
    "\n",
    "También he añadido una sección en la que controlo si el dispositivo con el que se va a entrenar es la GPU, pongo un creciomiento progresivo en el uso de memoria para evitar sobrecarga y también un límite para evitar de nuevo el OOM.\n",
    "\n",
    "La línea tf.config.optimizer.set_jit(False) la usaba cuando tenía desactivado el XLA para evitar así que compilase por XLA y evitar posibles problemas de rendimiento, pero ese no era el problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c038a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import tensorflow as tf, gc, logging\n",
    "tf.config.optimizer.set_jit(False) \n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(\"CUDA:\",   info[\"cuda_version\"])\n",
    "print(\"cuDNN:\",  info[\"cudnn_version\"])\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "#import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "# Fijamos la semilla para reproducibilidad\n",
    "SEED = 42\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "#tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED) \n",
    "tf.config.experimental.enable_op_determinism() # Para evitar problemas de determinismo en TensorFlow \n",
    "\n",
    "# Hiperparámetros\n",
    "NUM_LABELS = 44  # 43 emociones + 1 sin emoción\n",
    "\n",
    "#MODEL_NAME = \"monologg/kobert\" #\"beomi/KcELECTRA-base\" \"monologg/kobert\"\n",
    "AVAILABLE_MODELS = {\n",
    "    \"kc-electra\":         \"beomi/KcELECTRA-base\", # Testeado\n",
    "    \"koelectra-v3\":       \"monologg/koelectra-base-v3-discriminator\",\n",
    "    \"bert-kor\":           \"kykim/bert-kor-base\", # Testeado\n",
    "    \"kobert\":             \"skt/kobert-base-v1\",\n",
    "    \"klue-roberta\":       \"klue/roberta-base\", # Testeado\n",
    "    \"xlm-roberta\":        \"xlm-roberta-base\"\n",
    "}\n",
    "\n",
    "# Selecciona uno\n",
    "MODEL_NAME = AVAILABLE_MODELS[\"klue-roberta\"]\n",
    "\n",
    "MAX_LENGTH = 256 # Longitud máxima de las secuencias\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DROPOUT_RATE   = 0.3      \n",
    "L2_REG         = 1e-5         \n",
    "UNFREEZE_EPOCH = 3        \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "BASE_LR = 1e-4\n",
    "FT_LR = 2e-5\n",
    "WEIGHT_DECAY   = 0.01    # Decaimiento de pesos \n",
    "BETA_1         = 0.9     # Parámetro β₁ de AdamW\n",
    "BETA_2         = 0.999   # Parámetro β₂ de AdamW\n",
    "EPSILON        = 1e-6    # Epsilon de AdamW para estabilidad numérica\n",
    "\n",
    "SHUFFLE_BUFFER = 5_000 # Limitamos el tamaño del buffer de shuffle para evitar OOM \n",
    "\n",
    "# Forzar el uso de la GPU y activamos el crecimiento de memoria y la limitamo para evitar el OOM\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)]\n",
    "    )\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(\"Dispositivo:\", device_name)\n",
    "    \n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcd85a",
   "metadata": {},
   "source": [
    "### 1.2 Previsualización de los datos de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37648813",
   "metadata": {},
   "source": [
    "Aquí he cargado los datasets de manera manual, pensé en hacerlo descargando directamente desde huggingface como hacen en el código de KOTE, pero ya que tenía los archivos quise probar a hacerlo así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from datasets import load_dataset\n",
    "\n",
    "# Cargo los datasets en local pero también podría ser desde HuggingFace como en el notebook que da KOTE: dataset = load_dataset(\"searle-j/kote\")\n",
    "train_path = \"train.tsv\"\n",
    "val_path   = \"val.tsv\"    \n",
    "test_path  = \"test.tsv\"\n",
    "\n",
    "columns = [\"id\", \"text\", \"labels\"] \n",
    "df_train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=columns)\n",
    "df_val   = pd.read_csv(val_path,   sep=\"\\t\", header=None, names=columns)\n",
    "df_test  = pd.read_csv(test_path,  sep=\"\\t\", header=None, names=columns)\n",
    "\n",
    "print(f\"Ejemplos cargados de Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d9fe3",
   "metadata": {},
   "source": [
    "#### 1.2.1 Control de sesgos de género"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e781cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Definimos el mapeo de términos de género\n",
    "gender_map = {\n",
    "    \"여자\":       \"남자\",      # mujer -> hombre\n",
    "    \"남자\":       \"여자\",      # hombre -> mujer\n",
    "    \"여성\":       \"남성\",      # femenino -> masculino\n",
    "    \"남성\":       \"여성\",      # masculino -> femenino\n",
    "\n",
    "    \"아버지\":     \"어머니\",    # padre -> madre\n",
    "    \"어머니\":     \"아버지\",    # madre -> padre\n",
    "    \"아들\":       \"딸\",        # hijo -> hija\n",
    "    \"딸\":         \"아들\",      # hija -> hijo\n",
    "    \"남편\":       \"아내\",      # esposo -> esposa\n",
    "    \"아내\":       \"남편\",      # esposa -> esposo\n",
    "    \"오빠\":       \"언니\",      # hermano mayor (hablante femenino) -> hermana mayor\n",
    "    \"언니\":       \"오빠\",      # hermana mayor -> hermano mayor (hablante femenino)\n",
    "    \"형\":         \"누나\",      # hermano mayor (hablante masculino) -> hermana mayor\n",
    "    \"누나\":       \"형\",        # hermana mayor -> hermano mayor (hablante masculino)\n",
    "\n",
    "    \"남자친구\":   \"여자친구\",  # novio -> novia\n",
    "    \"여자친구\":   \"남자친구\",  # novia -> novio\n",
    "    \"총각\":       \"처녀\",      # soltero -> soltera\n",
    "    \"처녀\":       \"총각\",      # soltera -> soltero\n",
    "\n",
    "    \"왕자\":       \"공주\",      # príncipe -> princesa\n",
    "    \"공주\":       \"왕자\",      # princesa -> príncipe\n",
    "    \"왕\":         \"여왕\",      # rey -> reina\n",
    "    \"여왕\":       \"왕\",        # reina -> rey\n",
    "\n",
    "    \"남배우\":     \"여배우\",    # actor -> actriz\n",
    "    \"여배우\":     \"남배우\",    # actriz -> actor\n",
    "\n",
    "    \"그는\":       \"그녀는\",    # él (sujeto) -> ella (sujeto)\n",
    "    \"그녀는\":     \"그는\",      # ella (sujeto) -> él (sujeto)\n",
    "    \"그를\":       \"그녀를\",    # lo/le (objeto) -> la/le (objeto)\n",
    "    \"그녀를\":     \"그를\",      # la/le (objeto) -> lo/le (objeto)\n",
    "    \"그의\":       \"그녀의\",    # su (masculino) -> su (femenino)\n",
    "    \"그녀의\":     \"그의\",      # su (femenino) -> su (masculino)\n",
    "\n",
    "    \"남성적\":     \"여성적\",    # masculino (adjetivo) -> femenino (adjetivo)\n",
    "    \"여성적\":     \"남성적\",    # femenino (adjetivo) -> masculino (adjetivo)\n",
    "}\n",
    "\n",
    "\n",
    "# Identificamos las filas cuyos textos contienen alguna clave de gender_map\n",
    "pattern = \"|\".join(map(re.escape, gender_map.keys()))\n",
    "mask = df_train['text'].str.contains(pattern)\n",
    "\n",
    "# Creamos un DataFrame con las filas a gender-swappear\n",
    "df_swapped = df_train[mask].copy()\n",
    "\n",
    "# Aplicamos el reemplazo en la columna de texto\n",
    "def swap_gender_tokens(txt):\n",
    "    for src, tgt in gender_map.items():\n",
    "        txt = txt.replace(src, tgt)\n",
    "    return txt\n",
    "\n",
    "df_swapped['text'] = df_swapped['text'].apply(swap_gender_tokens)\n",
    "\n",
    "# Concatenamos y barajamos el DataFrame resultante antes del split\n",
    "df_train = pd.concat([df_train, df_swapped], ignore_index=True)\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Añadidos {len(df_swapped)} ejemplos de género intercambiado. Nuevo tamaño de df_train: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8b35a",
   "metadata": {},
   "source": [
    "### 1.3 Binarización de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Para convertir la columna labels de string a lista de ints\n",
    "def parse_labels(label_str):\n",
    "    if pd.isna(label_str) or label_str == \"\":\n",
    "        return []\n",
    "    return [int(x) for x in label_str.split(\",\")]\n",
    "\n",
    "train_label_lists = df_train[\"labels\"].apply(parse_labels)\n",
    "val_label_lists   = df_val[\"labels\"].apply(parse_labels)\n",
    "test_label_lists  = df_test[\"labels\"].apply(parse_labels)\n",
    "\n",
    "# Pasamos la lisa de etiquetas a un formato multi-hot\n",
    "# (una lista de listas de etiquetas, donde cada lista tiene el mismo tamaño que el número total de etiquetas)\n",
    "mlb = MultiLabelBinarizer(classes=list(range(NUM_LABELS)))\n",
    "mlb.fit(train_label_lists)\n",
    "\n",
    "y_train = mlb.transform(train_label_lists)\n",
    "y_val   = mlb.transform(val_label_lists)\n",
    "y_test  = mlb.transform(test_label_lists)\n",
    "\n",
    "print(\"Tamaño de y_train:\", y_train.shape)\n",
    "print(\"Ejemplo de vector de etiquetas (multi-hot) para una muestra:\\n\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa790da",
   "metadata": {},
   "source": [
    "### 1.4 Revisión de los comentarios y pasarlos a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos los comentarios también a listas\n",
    "train_texts = df_train[\"text\"].tolist()\n",
    "val_texts   = df_val[\"text\"].tolist()\n",
    "test_texts  = df_test[\"text\"].tolist()\n",
    "\n",
    "print(\"Texto de ejemplo:\", train_texts[0])\n",
    "print(\"Etiquetas de ejemplo:\", train_label_lists.iloc[0])\n",
    "print(\"Vector multi-hot:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd2e0c",
   "metadata": {},
   "source": [
    "### 1.5 Definición del tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359be512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "# Cargamos el tokenizador del modelo preentrenado \n",
    "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from kobert_transformers import get_tokenizer as get_kobert_tokenizer\n",
    "\n",
    "def load_tokenizer(model_name):\n",
    "    if \"kobert\" in model_name.lower():\n",
    "        tokenizer = get_kobert_tokenizer()\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.model_max_length \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "tokenizer = tokenizer = load_tokenizer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a374",
   "metadata": {},
   "source": [
    "## 2. Definición del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69031ccc",
   "metadata": {},
   "source": [
    "### 2.1 Carga del modelo preentrenado de transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9adf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import TFAutoModel, AutoConfig\n",
    "\n",
    "#config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "#transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ef877",
   "metadata": {},
   "source": [
    "### 2.2 Pooling de Representaciones y Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, TFAutoModel, TFBertModel\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_model_and_tokenizer(model_name, num_labels, max_length):\n",
    "    \"\"\"\n",
    "    Carga tokenizador, configuración y modelo, y construye arquitectura de clasificación multilabel.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"kobert\" in model_name.lower():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        base_model = TFBertModel.from_pretrained(model_name, config=config, from_pt=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        base_model = TFAutoModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "    # Entradas\n",
    "    input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    outputs = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    if hasattr(outputs, \"pooler_output\") and outputs.pooler_output is not None:\n",
    "        x = outputs.pooler_output\n",
    "    else:\n",
    "        x = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(L2_REG))(x)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    logits = tf.keras.layers.Dense(num_labels, activation=\"sigmoid\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(L2_REG))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "    return tokenizer, model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from transformers import TFAutoModel, AutoConfig\n",
    "\n",
    "def build_model():\n",
    "    # Cargamos el modelo preentrenado y su configuración\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    transformer = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "    # Definimos la arquitectura del modelo\n",
    "    input_ids     = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # Definimos la salida\n",
    "    outputs = transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    sequence_output = outputs.last_hidden_state\n",
    "\n",
    "    # Capa de mean pooling \n",
    "    pooled_output = tf.reduce_mean(sequence_output, axis=1)\n",
    "\n",
    "    # Capa densa con regularización L2 y dropout para evitar el overfitting \n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "                             )(pooled_output)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "    # Capa de salida con activación sigmoide para clasificación multi-etiqueta\n",
    "    logits = tf.keras.layers.Dense(NUM_LABELS, activation=\"sigmoid\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "                                  )(x)\n",
    "    \n",
    "    # Mdelo keras final \n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "    return model, transformer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bc188",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02feec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Función generadora de datos para tf.data.Dataset ──\n",
    "def data_generator(texts, labels, tokenizer):\n",
    "    for text, label in zip(texts, labels):\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=False\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "        yield (input_ids, attention_mask), label\n",
    "\n",
    "# ── Tipos y formas de la salida para from_generator ──\n",
    "output_types = ((tf.int32, tf.int32), tf.int32)\n",
    "output_shapes = ((tf.TensorShape([None]), tf.TensorShape([None])), tf.TensorShape([NUM_LABELS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36740eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una métrica personalizada para F1 micro con keras metrics para clasificación multi-etiqueta \n",
    "class MicroF1(tf.keras.metrics.Metric): \n",
    "    def __init__(self, num_labels, threshold=0.5, name=\"f1_micro\", **kwargs):\n",
    "        # Inicializamos la métrica\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_labels = num_labels\n",
    "        self.threshold  = threshold\n",
    "\n",
    "        # Inicializamos los contadores para true positives, false positives y false negatives\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convertimos las predicciones a binario según el umbral\n",
    "        y_pred = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        # Calculamos los verdaderos positivos, falsos positivos y falsos negativos\n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "\n",
    "        # Actualizamos los contadores\n",
    "        self.tp.assign_add(tp)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.fn.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        # Calculamos la métrica F1 micro\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        recall    = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Reiniciamos los contadores\n",
    "        for v in (self.tp, self.fp, self.fn):\n",
    "            v.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_ckpt_dir = os.path.join(\"checkpoints\", run_ts)\n",
    "os.makedirs(base_ckpt_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def print_mem(which):\n",
    "    p = psutil.Process(os.getpid())\n",
    "    rss = p.memory_info().rss/1024**2\n",
    "    gpu = tf.config.experimental.get_memory_info(\"GPU:0\")[\"current\"]/1024**2\n",
    "    print(f\"[{which}] RAM = {rss:.1f} MB, GPU = {gpu:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import re\n",
    "from tensorflow.data import experimental as tf_exp\n",
    "\n",
    "texts_all  = train_texts + val_texts\n",
    "labels_all = np.vstack([y_train, y_val])  # shape = (len(train)+len(val), NUM_LABELS)\n",
    "\n",
    "# ── Preparar CV ──\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "fold_aucs = []\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/kote\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ── Loop de folds ──\n",
    "for fold, (train_idx, val_idx) in enumerate(mskf.split(texts_all, labels_all), start=1):\n",
    "\n",
    "    # ── Limpieza de memoria antes de cada fold ──\n",
    "    print_mem(f\"fold {fold} — antes clear\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    print_mem(f\"fold {fold} — tras clear & gc\")\n",
    "\n",
    "    print(f\"\\n>>> Fold {fold}\")\n",
    "\n",
    "    # ── Definición de rutas de cache y nombre de los checkpoints ──\n",
    "    cache_file = os.path.join(cache_dir, f\"fold{fold}.tf-data\")\n",
    "    clean_name = re.sub(r'[^A-Za-z0-9._-]', '_', MODEL_NAME)\n",
    "    checkpoint_path = os.path.join(base_ckpt_dir, f\"fold{fold}_{clean_name}.h5\")\n",
    "\n",
    "    # ── Partición de los textos y las etiquetas ──\n",
    "    X_tr = [texts_all[i] for i in train_idx]\n",
    "    y_tr = labels_all[train_idx]\n",
    "    X_va = [texts_all[i] for i in val_idx]\n",
    "    y_va = labels_all[val_idx]\n",
    "\n",
    "    # ── Calculamos la longitud de las secuencias ──\n",
    "    def seq_len_fn(inputs, labels): \n",
    "        return tf.shape(inputs[0])[0]\n",
    "    \n",
    "    # ── Construimos los dataset para este fold ──\n",
    "    tokenizer, model, transformer_model = load_model_and_tokenizer(MODEL_NAME, NUM_LABELS, MAX_LENGTH)\n",
    "\n",
    "    # Dataset train\n",
    "    train_ds = (\n",
    "        tf.data.Dataset\n",
    "        .from_generator(lambda: data_generator(X_tr, y_tr, tokenizer),\n",
    "                        output_types=output_types,\n",
    "                        output_shapes=output_shapes)\n",
    "        .cache(cache_file)\n",
    "        .shuffle(SHUFFLE_BUFFER, seed=SEED)\n",
    "        .apply(\n",
    "            tf_exp.bucket_by_sequence_length(\n",
    "                element_length_func=seq_len_fn,\n",
    "                bucket_boundaries=[64, 128, 192],\n",
    "                bucket_batch_sizes=[BATCH_SIZE*2, BATCH_SIZE, BATCH_SIZE//2, max(1, BATCH_SIZE//4)],\n",
    "                padded_shapes=(([None], [None]), [NUM_LABELS]),\n",
    "                padding_values=((tokenizer.pad_token_id, 0), 0),\n",
    "                drop_remainder=False\n",
    "            )\n",
    "        )\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # Dataset val\n",
    "    val_ds = (\n",
    "        tf.data.Dataset\n",
    "        .from_generator(lambda: data_generator(X_va, y_va, tokenizer),\n",
    "                        output_types=output_types,\n",
    "                        output_shapes=output_shapes)\n",
    "        .padded_batch(\n",
    "            BATCH_SIZE,\n",
    "            padded_shapes=(([MAX_LENGTH], [MAX_LENGTH]), [NUM_LABELS]),\n",
    "            padding_values=((tokenizer.pad_token_id, 0), 0)\n",
    "        )\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "    # ── Definición de callbacks ──\n",
    "    early_stopping_cb = EarlyStopping(monitor=\"val_f1_micro\", mode=\"max\", patience=3, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_cb = ReduceLROnPlateau(monitor=\"val_f1_micro\", mode=\"max\", factor=0.5, patience=2, verbose=1)\n",
    "    checkpoint_cb = ModelCheckpoint(filepath=checkpoint_path, monitor=\"val_f1_micro\", save_best_only=True, save_freq='epoch',mode=\"max\", verbose=1)\n",
    "\n",
    "    callbacks_fold = [early_stopping_cb, reduce_lr_cb, checkpoint_cb]\n",
    "\n",
    "    # ── Métricas ──\n",
    "    metrics= [AUC(name=\"AUC\", multi_label=True), MicroF1(num_labels=NUM_LABELS, threshold=0.5, name=\"f1_micro\")] \n",
    "\n",
    "    # ── Construimos el modelo desde cero ──\n",
    "    #model, transformer_model = build_model()\n",
    "\n",
    "    # ── Pre‐entrenamiento de la cabeza ──\n",
    "    opt1 = AdamW( learning_rate=BASE_LR, weight_decay=WEIGHT_DECAY, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "    opt1 = LossScaleOptimizer(opt1)\n",
    "\n",
    "    transformer_model.trainable = False\n",
    "    model.compile( optimizer=opt1, loss=\"binary_crossentropy\", metrics= metrics)\n",
    "    model.fit( train_ds, validation_data=val_ds, epochs=UNFREEZE_EPOCH-1, callbacks=callbacks_fold, verbose=1)\n",
    "\n",
    "    # ── Fine-tuning completo ──\n",
    "    opt2 = AdamW( learning_rate=FT_LR, weight_decay=WEIGHT_DECAY, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "    opt2 = LossScaleOptimizer(opt2)\n",
    "\n",
    "    transformer_model.trainable = True\n",
    "    model.compile(optimizer=opt2, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "    model.fit(train_ds, validation_data=val_ds, initial_epoch=UNFREEZE_EPOCH-1, epochs=EPOCHS, callbacks=callbacks_fold, verbose=1)\n",
    "\n",
    "    # ── Evaluación de este fold ──\n",
    "    m = model.evaluate(val_ds, return_dict=True)\n",
    "    print(f\"Fold {fold} — AUC:\", m[\"AUC\"])\n",
    "    fold_aucs.append(m[\"AUC\"])\n",
    "\n",
    "    # ── Limpieza de objetos del fold ──\n",
    "    del train_ds, val_ds, model, transformer_model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0')\n",
    "    print_mem(f\"fold {fold} — tras limpieza\")\n",
    "\n",
    "# ── Resultados finales de la CV ──\n",
    "print(\"\\nAUC por fold:\", fold_aucs)\n",
    "print(\"Media ± std:\", np.mean(fold_aucs), \"±\", np.std(fold_aucs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
