{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03fdc7",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ed6c0",
   "metadata": {},
   "source": [
    "### 1.1 Preparación del entorno "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf379748",
   "metadata": {},
   "source": [
    "Aquí lo que hago es preaprar los imports principales y las variables de entorno. Ahora están comentadas porque como te dije en el correo estaba teniendo problemas con la versión de Cuda y Keras. Ahora lo he solucionado, pero basicamente lo que pasaba es que estaba usando una versión de Keras (Keras 3) la cual no es compatible con transfomers, al final lo soluciones activando  la variable de entorno TF_USE_LEGACY_KERAS e instalando en mi env de conda tf_keras para tener Keras 2. \n",
    "\n",
    "La variable TF_ENABLE_XLA la desactivé porque se supone que podía ser una causa de un problema de OOM que estaba teniendo durante el entrenamiento, pero al final resultó ser un problema con las versiones de las librerías.\n",
    "\n",
    "La variables de entorno que tienen que ver con CUDA era porque creía que mi entorno virtual de conda (el cual al final lo he tenido que meter en un WLS2 con ubuntu porque en windows estaba teniendo problemas de compatibilidad peores, estaba cogienod) estaba cogiendo la versión de CUDA que no era, porque tenía varias instaladas. Pero era más un fallo de configuración del entorno que eso.\n",
    "\n",
    "También he añadido una sección en la que controlo si el dispositivo con el que se va a entrenar es la GPU, pongo un creciomiento progresivo en el uso de memoria para evitar sobrecarga y también un límite para evitar de nuevo el OOM.\n",
    "\n",
    "La línea tf.config.optimizer.set_jit(False) la usaba cuando tenía desactivado el XLA para evitar así que compilase por XLA y evitar posibles problemas de rendimiento, pero ese no era el problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c038a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import tensorflow as tf, gc, logging\n",
    "tf.config.optimizer.set_jit(False) \n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(\"CUDA:\",   info[\"cuda_version\"])\n",
    "print(\"cuDNN:\",  info[\"cudnn_version\"])\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "#import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "# Fijamos la semilla para reproducibilidad\n",
    "SEED = 42\n",
    "#random.seed(SEED)\n",
    "#np.random.seed(SEED)\n",
    "#tf.random.set_seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED) \n",
    "tf.config.experimental.enable_op_determinism() # Para evitar problemas de determinismo en TensorFlow \n",
    "\n",
    "# Hiperparámetros\n",
    "NUM_LABELS = 44  # 43 emociones + 1 sin emoción\n",
    "\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\" #\"beomi/KcELECTRA-base\" \"monologg/kobert\"\n",
    "MAX_LENGTH = 256 # Longitud máxima de las secuencias\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "DROPOUT_RATE   = 0.3      \n",
    "L2_REG         = 1e-5         \n",
    "UNFREEZE_EPOCH = 3        \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "BASE_LR = 1e-4\n",
    "FT_LR = 2e-5\n",
    "WEIGHT_DECAY   = 0.01    # Decaimiento de pesos \n",
    "BETA_1         = 0.9     # Parámetro β₁ de AdamW\n",
    "BETA_2         = 0.999   # Parámetro β₂ de AdamW\n",
    "EPSILON        = 1e-6    # Epsilon de AdamW para estabilidad numérica\n",
    "\n",
    "SHUFFLE_BUFFER = 10_000 # Limitamos el tamaño del buffer de shuffle para evitar OOM \n",
    "\n",
    "# Forzar el uso de la GPU y activamos el crecimiento de memoria y la limitamo para evitar el OOM\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)]\n",
    "    )\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(\"Dispositivo:\", device_name)\n",
    "    \n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcd85a",
   "metadata": {},
   "source": [
    "### 1.2 Previsualización de los datos de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37648813",
   "metadata": {},
   "source": [
    "Aquí he cargado los datasets de manera manual, pensé en hacerlo descargando directamente desde huggingface como hacen en el código de KOTE, pero ya que tenía los archivos quise probar a hacerlo así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from datasets import load_dataset\n",
    "\n",
    "# Cargo los datasets en local pero también podría ser desde HuggingFace como en el notebook que da KOTE: dataset = load_dataset(\"searle-j/kote\")\n",
    "\n",
    "train_path = \"train.tsv\"\n",
    "val_path   = \"val.tsv\"    \n",
    "test_path  = \"test.tsv\"\n",
    "\n",
    "columns = [\"id\", \"text\", \"labels\"] \n",
    "df_train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=columns)\n",
    "df_val   = pd.read_csv(val_path,   sep=\"\\t\", header=None, names=columns)\n",
    "df_test  = pd.read_csv(test_path,  sep=\"\\t\", header=None, names=columns)\n",
    "\n",
    "print(f\"Ejemplos cargados de Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d9fe3",
   "metadata": {},
   "source": [
    "#### 1.2.1 Control de sesgos de género"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e781cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Definimos el mapeo de términos de género\n",
    "gender_map = {\n",
    "    \"여자\":       \"남자\",      # mujer -> hombre\n",
    "    \"남자\":       \"여자\",      # hombre -> mujer\n",
    "    \"여성\":       \"남성\",      # femenino -> masculino\n",
    "    \"남성\":       \"여성\",      # masculino -> femenino\n",
    "\n",
    "    \"아버지\":     \"어머니\",    # padre -> madre\n",
    "    \"어머니\":     \"아버지\",    # madre -> padre\n",
    "    \"아들\":       \"딸\",        # hijo -> hija\n",
    "    \"딸\":         \"아들\",      # hija -> hijo\n",
    "    \"남편\":       \"아내\",      # esposo -> esposa\n",
    "    \"아내\":       \"남편\",      # esposa -> esposo\n",
    "    \"오빠\":       \"언니\",      # hermano mayor (hablante femenino) -> hermana mayor\n",
    "    \"언니\":       \"오빠\",      # hermana mayor -> hermano mayor (hablante femenino)\n",
    "    \"형\":         \"누나\",      # hermano mayor (hablante masculino) -> hermana mayor\n",
    "    \"누나\":       \"형\",        # hermana mayor -> hermano mayor (hablante masculino)\n",
    "\n",
    "    \"남자친구\":   \"여자친구\",  # novio -> novia\n",
    "    \"여자친구\":   \"남자친구\",  # novia -> novio\n",
    "    \"총각\":       \"처녀\",      # soltero -> soltera\n",
    "    \"처녀\":       \"총각\",      # soltera -> soltero\n",
    "\n",
    "    \"왕자\":       \"공주\",      # príncipe -> princesa\n",
    "    \"공주\":       \"왕자\",      # princesa -> príncipe\n",
    "    \"왕\":         \"여왕\",      # rey -> reina\n",
    "    \"여왕\":       \"왕\",        # reina -> rey\n",
    "\n",
    "    \"남배우\":     \"여배우\",    # actor -> actriz\n",
    "    \"여배우\":     \"남배우\",    # actriz -> actor\n",
    "\n",
    "    \"그는\":       \"그녀는\",    # él (sujeto) -> ella (sujeto)\n",
    "    \"그녀는\":     \"그는\",      # ella (sujeto) -> él (sujeto)\n",
    "    \"그를\":       \"그녀를\",    # lo/le (objeto) -> la/le (objeto)\n",
    "    \"그녀를\":     \"그를\",      # la/le (objeto) -> lo/le (objeto)\n",
    "    \"그의\":       \"그녀의\",    # su (masculino) -> su (femenino)\n",
    "    \"그녀의\":     \"그의\",      # su (femenino) -> su (masculino)\n",
    "\n",
    "    \"남성적\":     \"여성적\",    # masculino (adjetivo) -> femenino (adjetivo)\n",
    "    \"여성적\":     \"남성적\",    # femenino (adjetivo) -> masculino (adjetivo)\n",
    "}\n",
    "\n",
    "\n",
    "# Identificamos las filas cuyos textos contienen alguna clave de gender_map\n",
    "pattern = \"|\".join(map(re.escape, gender_map.keys()))\n",
    "mask = df_train['text'].str.contains(pattern)\n",
    "\n",
    "# Creamos un DataFrame con las filas a gender-swappear\n",
    "df_swapped = df_train[mask].copy()\n",
    "\n",
    "# Aplicamos el reemplazo en la columna de texto\n",
    "def swap_gender_tokens(txt):\n",
    "    for src, tgt in gender_map.items():\n",
    "        txt = txt.replace(src, tgt)\n",
    "    return txt\n",
    "\n",
    "df_swapped['text'] = df_swapped['text'].apply(swap_gender_tokens)\n",
    "\n",
    "# Concatenamos y barajamos el DataFrame resultante antes del split\n",
    "df_train = pd.concat([df_train, df_swapped], ignore_index=True)\n",
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Añadidos {len(df_swapped)} ejemplos de género intercambiado. Nuevo tamaño de df_train: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8b35a",
   "metadata": {},
   "source": [
    "### 1.3 Binarización de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Para convertir la columna labels de string a lista de ints\n",
    "def parse_labels(label_str):\n",
    "    if pd.isna(label_str) or label_str == \"\":\n",
    "        return []\n",
    "    return [int(x) for x in label_str.split(\",\")]\n",
    "\n",
    "train_label_lists = df_train[\"labels\"].apply(parse_labels)\n",
    "val_label_lists   = df_val[\"labels\"].apply(parse_labels)\n",
    "test_label_lists  = df_test[\"labels\"].apply(parse_labels)\n",
    "\n",
    "# Pasamos la lisa de etiquetas a un formato multi-hot\n",
    "# (una lista de listas de etiquetas, donde cada lista tiene el mismo tamaño que el número total de etiquetas)\n",
    "mlb = MultiLabelBinarizer(classes=list(range(NUM_LABELS)))\n",
    "mlb.fit(train_label_lists)\n",
    "\n",
    "y_train = mlb.transform(train_label_lists)\n",
    "y_val   = mlb.transform(val_label_lists)\n",
    "y_test  = mlb.transform(test_label_lists)\n",
    "\n",
    "print(\"Tamaño de y_train:\", y_train.shape)\n",
    "print(\"Ejemplo de vector de etiquetas (multi-hot) para una muestra:\\n\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa790da",
   "metadata": {},
   "source": [
    "### 1.4 Revisión de los comentarios y pasarlos a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos los comentarios también a listas\n",
    "train_texts = df_train[\"text\"].tolist()\n",
    "val_texts   = df_val[\"text\"].tolist()\n",
    "test_texts  = df_test[\"text\"].tolist()\n",
    "\n",
    "print(\"Texto de ejemplo:\", train_texts[0])\n",
    "print(\"Etiquetas de ejemplo:\", train_label_lists.iloc[0])\n",
    "print(\"Vector multi-hot:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd2e0c",
   "metadata": {},
   "source": [
    "### 1.5 Definición del tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359be512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a374",
   "metadata": {},
   "source": [
    "## 2. Definición del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69031ccc",
   "metadata": {},
   "source": [
    "### 2.1 Carga del modelo preentrenado de transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9adf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ef877",
   "metadata": {},
   "source": [
    "### 2.2 Pooling de Representaciones y Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "\n",
    "def build_model():\n",
    "    # 1) Carga limpia del transformer\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    transformer = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    # 2) Entradas\n",
    "    input_ids     = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    # 3) Forward + pooling (aquí usamos mean pooling; puedes cambiar a CLS token si lo prefieres)\n",
    "    outputs = transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    sequence_output = outputs.last_hidden_state\n",
    "    pooled_output = tf.reduce_mean(sequence_output, axis=1)\n",
    "    # 4) Cabeza de clasificación\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "                             )(pooled_output)\n",
    "    x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "    logits = tf.keras.layers.Dense(NUM_LABELS, activation=\"sigmoid\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(L2_REG)\n",
    "                                  )(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "    return model, transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bc188",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02feec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Función generadora de datos para tf.data.Dataset ──\n",
    "def data_generator(texts, labels):\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Tokeniza sin padding fijo; el padding lo hará el batch\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=False\n",
    "        )\n",
    "        input_ids     = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "        yield (input_ids, attention_mask), label\n",
    "\n",
    "# ── Tipos y formas de la salida para from_generator ──\n",
    "output_types = (\n",
    "    (tf.int32, tf.int32),  # (input_ids, attention_mask)\n",
    "    tf.int32              # label multietiqueta\n",
    ")\n",
    "\n",
    "output_shapes = (\n",
    "    (tf.TensorShape([None]), tf.TensorShape([None])),  # secuencia variable\n",
    "    tf.TensorShape([NUM_LABELS])                       # vector de etiquetas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36740eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_labels, threshold=0.5, name=\"f1_micro\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_labels = num_labels\n",
    "        self.threshold  = threshold\n",
    "        # estados para TP, FP, FN\n",
    "        self.tp = self.add_weight(name=\"tp\", initializer=\"zeros\")\n",
    "        self.fp = self.add_weight(name=\"fp\", initializer=\"zeros\")\n",
    "        self.fn = self.add_weight(name=\"fn\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # binariza según threshold\n",
    "        y_pred = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        # acumula true positives, false positives, false negatives\n",
    "        tp = tf.reduce_sum(y_true * y_pred)\n",
    "        fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "        fn = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "        self.tp.assign_add(tp)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.fn.assign_add(fn)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.tp / (self.tp + self.fp + 1e-7)\n",
    "        recall    = self.tp / (self.tp + self.fn + 1e-7)\n",
    "        return 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "    def reset_state(self):\n",
    "        for v in (self.tp, self.fp, self.fn):\n",
    "            v.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "base_ckpt_dir = os.path.join(\"checkpoints\", run_ts)\n",
    "os.makedirs(base_ckpt_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def print_mem(which):\n",
    "    p = psutil.Process(os.getpid())\n",
    "    rss = p.memory_info().rss/1024**2\n",
    "    gpu = tf.config.experimental.get_memory_info(\"GPU:0\")[\"current\"]/1024**2\n",
    "    print(f\"[{which}] RAM = {rss:.1f} MiB, GPU = {gpu:.1f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1) Combinar train + val ──\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.mixed_precision import LossScaleOptimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import re\n",
    "from tensorflow.data import experimental as tf_exp\n",
    "\n",
    "texts_all  = train_texts + val_texts\n",
    "labels_all = np.vstack([y_train, y_val])  # shape = (len(train)+len(val), NUM_LABELS)\n",
    "\n",
    "# ── 2) Preparar CV ──\n",
    "mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "fold_aucs = []\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/kote\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# ── 3) Loop de folds ──\n",
    "for fold, (train_idx, val_idx) in enumerate(mskf.split(texts_all, labels_all), start=1):\n",
    "    print_mem(f\"fold {fold} — antes clear\")\n",
    "    tf.keras.backend.clear_session() # Limpiamos la sesión de Keras para evitar problemas de memoria\n",
    "    gc.collect() # Liberamos memoria de objetos no referenciados (recolección de basura)\n",
    "    print_mem(f\"fold {fold} — tras clear & gc\")\n",
    "    print(f\"\\n>>> Fold {fold}\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, f\"fold{fold}.tf-data\")\n",
    "\n",
    "    # Partición de textos y etiquetas\n",
    "    X_tr = [texts_all[i] for i in train_idx]\n",
    "    y_tr = labels_all[train_idx]\n",
    "    X_va = [texts_all[i] for i in val_idx]\n",
    "    y_va = labels_all[val_idx]\n",
    "\n",
    "    # Construir tf.data.Dataset para este fold\n",
    "    def seq_len_fn(inputs, labels):\n",
    "        # inputs es (ids, mask)\n",
    "        return tf.shape(inputs[0])[0]\n",
    "    \n",
    "    train_ds = (\n",
    "        tf.data.Dataset\n",
    "        .from_generator(lambda: data_generator(X_tr, y_tr),\n",
    "                        output_types=output_types,\n",
    "                        output_shapes=output_shapes)\n",
    "        .cache(cache_file)   # ← cacheamos la tokenización\n",
    "        .shuffle(SHUFFLE_BUFFER, seed=SEED)\n",
    "        .apply(\n",
    "            tf_exp.bucket_by_sequence_length(\n",
    "                element_length_func=seq_len_fn,\n",
    "                bucket_boundaries=[64, 128, 192],\n",
    "                bucket_batch_sizes=[BATCH_SIZE]*4,\n",
    "                padded_shapes=(([None], [None]), [NUM_LABELS]),\n",
    "                padding_values=((tokenizer.pad_token_id, 0), 0),\n",
    "                drop_remainder=False\n",
    "            )\n",
    "        )\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset\n",
    "        .from_generator(lambda: data_generator(X_va, y_va),\n",
    "                        output_types=output_types,\n",
    "                        output_shapes=output_shapes)\n",
    "        .cache()  # ← cacheamos también la validación\n",
    "        .padded_batch(\n",
    "            BATCH_SIZE,\n",
    "            padded_shapes=(([MAX_LENGTH], [MAX_LENGTH]), [NUM_LABELS]),\n",
    "            padding_values=((tokenizer.pad_token_id, 0), 0)\n",
    "        )\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "    # — Definición de callbacks —\n",
    "    clean_name = re.sub(r'[^A-Za-z0-9._-]', '_', MODEL_NAME)\n",
    "    checkpoint_path = os.path.join(base_ckpt_dir, f\"fold{fold}_{clean_name}.h5\")\n",
    "\n",
    "    early_stopping_cb = EarlyStopping(monitor=\"val_f1_micro\", mode=\"max\", patience=3, restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_cb = ReduceLROnPlateau(monitor=\"val_f1_micro\", mode=\"max\", factor=0.5, patience=2, verbose=1)\n",
    "    checkpoint_cb = ModelCheckpoint(filepath=checkpoint_path, monitor=\"val_f1_micro\", save_best_only=True, mode=\"max\", verbose=1)\n",
    "\n",
    "    callbacks_fold = [early_stopping_cb, reduce_lr_cb, checkpoint_cb]\n",
    "\n",
    "    metrics= [\n",
    "        AUC(name=\"AUC\", multi_label=True),\n",
    "        MicroF1(num_labels=NUM_LABELS, threshold=0.5, name=\"f1_micro\")\n",
    "    ] \n",
    "\n",
    "    # — Construir modelo desde cero —\n",
    "    model, transformer_model = build_model()\n",
    "\n",
    "    # — Pre‐entrenamiento de la cabeza —\n",
    "    opt1 = AdamW( learning_rate=BASE_LR, weight_decay=WEIGHT_DECAY, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "    opt1 = LossScaleOptimizer(opt1)\n",
    "\n",
    "    transformer_model.trainable = False\n",
    "    model.compile( optimizer=opt1, loss=\"binary_crossentropy\", metrics= metrics)\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=UNFREEZE_EPOCH-1,\n",
    "        callbacks=callbacks_fold,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # — Fine-tuning completo —\n",
    "    opt2 = AdamW( learning_rate=FT_LR, weight_decay=WEIGHT_DECAY, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "    opt2 = LossScaleOptimizer(opt2)\n",
    "\n",
    "    transformer_model.trainable = True\n",
    "    model.compile(optimizer=opt2, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "    \n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        initial_epoch=UNFREEZE_EPOCH-1,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks_fold,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluación de este fold\n",
    "    m = model.evaluate(val_ds, return_dict=True)\n",
    "    print(f\"Fold {fold} — AUC:\", m[\"AUC\"])\n",
    "    fold_aucs.append(m[\"AUC\"])\n",
    "\n",
    "    # ── Limpieza de objetos del fold ──\n",
    "    del train_ds, val_ds, model, transformer_model\n",
    "    gc.collect()\n",
    "    print_mem(f\"fold {fold} — tras delete\")\n",
    "\n",
    "\n",
    "# ── 4) Resultados finales de CV ──\n",
    "print(\"\\nAUC por fold:\", fold_aucs)\n",
    "print(\"Media ± std:\", np.mean(fold_aucs), \"±\", np.std(fold_aucs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
