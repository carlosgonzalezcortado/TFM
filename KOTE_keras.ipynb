{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03fdc7",
   "metadata": {},
   "source": [
    "## 1. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ed6c0",
   "metadata": {},
   "source": [
    "### 1.1 Preparación del entorno "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf379748",
   "metadata": {},
   "source": [
    "Aquí lo que hago es preaprar los imports principales y las variables de entorno. Ahora están comentadas porque como te dije en el correo estaba teniendo problemas con la versión de Cuda y Keras. Ahora lo he solucionado, pero basicamente lo que pasaba es que estaba usando una versión de Keras (Keras 3) la cual no es compatible con transfomers, al final lo soluciones activando  la variable de entorno TF_USE_LEGACY_KERAS e instalando en mi env de conda tf_keras para tener Keras 2. \n",
    "\n",
    "La variable TF_ENABLE_XLA la desactivé porque se supone que podía ser una causa de un problema de OOM que estaba teniendo durante el entrenamiento, pero al final resultó ser un problema con las versiones de las librerías.\n",
    "\n",
    "La variables de entorno que tienen que ver con CUDA era porque creía que mi entorno virtual de conda (el cual al final lo he tenido que meter en un WLS2 con ubuntu porque en windows estaba teniendo problemas de compatibilidad peores, estaba cogienod) estaba cogiendo la versión de CUDA que no era, porque tenía varias instaladas. Pero era más un fallo de configuración del entorno que eso.\n",
    "\n",
    "También he añadido una sección en la que controlo si el dispositivo con el que se va a entrenar es la GPU, pongo un creciomiento progresivo en el uso de memoria para evitar sobrecarga y también un límite para evitar de nuevo el OOM.\n",
    "\n",
    "La línea tf.config.optimizer.set_jit(False) la usaba cuando tenía desactivado el XLA para evitar así que compilase por XLA y evitar posibles problemas de rendimiento, pero ese no era el problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c038a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 10:45:02.164941: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-05 10:45:02.400233: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749113102.492993    1297 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749113102.521338    1297 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749113102.714781    1297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749113102.714831    1297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749113102.714833    1297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749113102.714834    1297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-05 10:45:02.740746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/carlo/miniconda3/envs/tf-gpu-v2/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: 12.5.1\n",
      "cuDNN: 9\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4090 Laptop GPU, compute capability 8.9\n",
      "Mixed precision policy: <Policy \"mixed_float16\">\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlo/miniconda3/envs/tf-gpu-v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: GPU\n",
      "TensorFlow version: 2.19.0\n",
      "Transformers version: 4.52.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "#os.environ['TF_ENABLE_XLA'] = '0'\n",
    "\n",
    "#os.environ['CUDA_HOME']       = '/usr/local/cuda'\n",
    "#os.environ['CUDA_PATH']       = '/usr/local/cuda'\n",
    "#os.environ['CUDA_ROOT']       = '/usr/local/cuda'\n",
    "#os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:' + os.environ.get('LD_LIBRARY_PATH','')\n",
    "\n",
    "import tensorflow as tf\n",
    "info = tf.sysconfig.get_build_info()\n",
    "print(\"CUDA:\",   info[\"cuda_version\"])\n",
    "print(\"cuDNN:\",  info[\"cudnn_version\"])\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "# Fijamos la semilla para reproducibilidad\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Hiperparámetros\n",
    "NUM_LABELS = 44  # 43 emociones + 1 sin emoción\n",
    "\n",
    "MODEL_NAME = \"monologg/kobert\" #\"beomi/KcELECTRA-base\" \n",
    "MAX_LENGTH = 512 # Longitud máxima de las secuencias\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DROPOUT_RATE   = 0.3      \n",
    "L2_REG         = 1e-5     \n",
    "WEIGHT_DECAY   = 1e-5     \n",
    "UNFREEZE_EPOCH = 3        \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Forzar el uso de la GPU y activamos el crecimiento de memoria y la limitamo para evitar el OOM\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=14336)]\n",
    "    )\n",
    "    device_name = \"GPU\"\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(\"Dispositivo:\", device_name)\n",
    "    \n",
    "#tf.config.optimizer.set_jit(False)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcd85a",
   "metadata": {},
   "source": [
    "### 1.2 Previsualización de los datos de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37648813",
   "metadata": {},
   "source": [
    "Aquí he cargado los datasets de manera manual, pensé en hacerlo descargando directamente desde huggingface como hacen en el código de KOTE, pero ya que tenía los archivos quise probar a hacerlo así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from datasets import load_dataset\n",
    "\n",
    "# Cargo los datasets en local pero también podría ser desde HuggingFace como en el notebook que da KOTE: dataset = load_dataset(\"searle-j/kote\")\n",
    "\n",
    "train_path = \"train.tsv\"\n",
    "val_path   = \"val.tsv\"    \n",
    "test_path  = \"test.tsv\"\n",
    "\n",
    "columns = [\"id\", \"text\", \"labels\"] \n",
    "df_train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=columns)\n",
    "df_val   = pd.read_csv(val_path,   sep=\"\\t\", header=None, names=columns)\n",
    "df_test  = pd.read_csv(test_path,  sep=\"\\t\", header=None, names=columns)\n",
    "\n",
    "print(f\"Ejemplos cargados de Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")\n",
    "\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a8b35a",
   "metadata": {},
   "source": [
    "### 1.3 Binarización de las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Para convertir la columna labels de string a lista de ints\n",
    "def parse_labels(label_str):\n",
    "    if pd.isna(label_str) or label_str == \"\":\n",
    "        return []\n",
    "    return [int(x) for x in label_str.split(\",\")]\n",
    "\n",
    "train_label_lists = df_train[\"labels\"].apply(parse_labels)\n",
    "val_label_lists   = df_val[\"labels\"].apply(parse_labels)\n",
    "test_label_lists  = df_test[\"labels\"].apply(parse_labels)\n",
    "\n",
    "# Pasamos la lisa de etiquetas a un formato multi-hot\n",
    "# (una lista de listas de etiquetas, donde cada lista tiene el mismo tamaño que el número total de etiquetas)\n",
    "mlb = MultiLabelBinarizer(classes=list(range(NUM_LABELS)))\n",
    "mlb.fit(train_label_lists)  # Con esto hacemos que el binarizador aprenda las etiquetas\n",
    "\n",
    "y_train = mlb.transform(train_label_lists)\n",
    "y_val   = mlb.transform(val_label_lists)\n",
    "y_test  = mlb.transform(test_label_lists)\n",
    "\n",
    "print(\"Tamaño de y_train:\", y_train.shape)\n",
    "print(\"Ejemplo de vector de etiquetas (multi-hot) para una muestra:\\n\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa790da",
   "metadata": {},
   "source": [
    "### 1.4 Revisión de los comentarios y pasarlos a listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos los comentarios también a listas\n",
    "train_texts = df_train[\"text\"].tolist()\n",
    "val_texts   = df_val[\"text\"].tolist()\n",
    "test_texts  = df_test[\"text\"].tolist()\n",
    "\n",
    "print(\"Texto de ejemplo:\", train_texts[0])\n",
    "print(\"Etiquetas de ejemplo:\", train_label_lists.iloc[0])\n",
    "print(\"Vector multi-hot:\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd2e0c",
   "metadata": {},
   "source": [
    "### 1.5 Definición del tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359be512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af257830",
   "metadata": {},
   "source": [
    "### 1.6 Preparación de los datasets para Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b42391",
   "metadata": {},
   "source": [
    "El tamaño de cada batch lo he puesto como último experimento en 32, no sé si 64 me daría OOM, pero como he estado teniendo los problemas que te he mendionado al principio he probado con 8, 16 y 32. \n",
    "\n",
    "También he optado por usar shuffle, padding y prefetch en el pipeline de datos para maximizar la eficiencia y la calidad del entrenamiento. El shuffle (con un buffer igual al tamaño del conjunto de train) garantiza que cada época reciba los ejemplos en un orden distinto, evitando correlaciones espurias y reduciendo el riesgo de sobreajuste. El padding dinámico, aplicado en cada batch, homogeneiza las secuencias a la longitud máxima del batch en lugar de usar un tamaño fijo global, lo que es más eficiente al rellenar solo lo estrictamente necesario. Finalmente, el prefetch solapa la preparación de los siguientes batches con la ejecución del modelo, de modo que mientras el GPU entrena con un batch, el pipeline ya está leyendo y procesando el siguiente, eliminando cuellos de botella y mejorando el rendimiento en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7142a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta función generamos los datasets a partir de los textos y etiquetas\n",
    "def data_generator(texts, labels):\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Tokenizamos el texto (sin padding fijo, lo hará el batch)\n",
    "        enc = tokenizer(text, truncation=True, max_length=MAX_LENGTH, padding=False)\n",
    "        input_ids = enc[\"input_ids\"] \n",
    "        attention_mask = enc[\"attention_mask\"] # La máscara de atención sirve para indicar qué tokens son padding y cuáles no\n",
    "        yield (input_ids, attention_mask), label\n",
    "\n",
    "# Definimos los tipos y formas de salida para el dataset necesario para el método from_generator\n",
    "output_types = ((tf.int32, tf.int32), tf.int32)\n",
    "output_shapes = ((tf.TensorShape([None]), tf.TensorShape([None])), tf.TensorShape([NUM_LABELS])) \n",
    "\n",
    "# Creamos los Dataset para train, val y test\n",
    "train_ds = tf.data.Dataset.from_generator(lambda: data_generator(train_texts, y_train), output_types=output_types, output_shapes=output_shapes)\n",
    "val_ds   = tf.data.Dataset.from_generator(lambda: data_generator(val_texts, y_val), output_types=output_types, output_shapes=output_shapes)\n",
    "test_ds  = tf.data.Dataset.from_generator(lambda: data_generator(test_texts, y_test), output_types=output_types, output_shapes=output_shapes)\n",
    "\n",
    "# Aplicamos shuffle, batch con padding y prefetch para optimizar el rendimiento\n",
    "train_ds = train_ds.shuffle(buffer_size=len(train_texts), seed=SEED, reshuffle_each_iteration=True) # (shuffle y el padding se aplican al dataset, no a los generadores)\n",
    "# Padding None para input_ids y attention_mask para que se ajusten a la longitud máxima del batch \n",
    "train_ds = train_ds.padded_batch(BATCH_SIZE, padded_shapes=(([None], [None]), [NUM_LABELS]), padding_values=((tokenizer.pad_token_id, 0), 0))\n",
    "# AUTOTUNE permite que TensorFlow ajuste automáticamente el número de threads para optimizar el rendimiento\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE) # Esto permite que el dataset se cargue en paralelo mientras el modelo entrena\n",
    "\n",
    "# Para el val y test no es necesario hacer shuffle ya que no se entrena con ellos \n",
    "val_ds = val_ds.padded_batch(BATCH_SIZE, padded_shapes=(([None], [None]), [NUM_LABELS]), padding_values=((tokenizer.pad_token_id, 0), 0))\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.padded_batch(BATCH_SIZE, padded_shapes=(([None], [None]), [NUM_LABELS]), padding_values=((tokenizer.pad_token_id, 0), 0))\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb6c39",
   "metadata": {},
   "source": [
    "### 1.7 Inspección de un batch de ejemplo del conjunto de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea42a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1): \n",
    "    (input_ids_batch, mask_batch), labels_batch = batch\n",
    "    print(\"Shape input_ids:\", input_ids_batch.shape)\n",
    "    print(\"Shape attention_mask:\", mask_batch.shape)\n",
    "    print(\"Shape labels:\", labels_batch.shape)\n",
    "    print(\"Ejemplo IDs primera muestra:\", input_ids_batch[0, :10].numpy())\n",
    "    print(\"Ejemplo mask primera muestra:\", mask_batch[0, :10].numpy())\n",
    "    print(\"Etiquetas primera muestra:\", labels_batch[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a374",
   "metadata": {},
   "source": [
    "## 2. Definición del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69031ccc",
   "metadata": {},
   "source": [
    "### 2.1 Carga del modelo preentrenado de transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9adf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "transformer_model = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "hidden_size = config.hidden_size \n",
    "print(f\"Modelo base '{MODEL_NAME}' cargado. Tamaño del hidden state: {hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ef877",
   "metadata": {},
   "source": [
    "### 2.2 Pooling de Representaciones y Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521a2fc",
   "metadata": {},
   "source": [
    "He optado por aplicar una capa de pooling para reducir la salida 3D del Transformer (batch, seq_len, hidden_size) a un vector 2D (batch, hidden_size) que sirva de entrada a la capa de clasificación. Y he implementado 2 posibilidades: CLS o Mean. Basicamente CLS es rápida y está optimizada para clasificación, mientras que Mean puede capturar mejor contextos largos. \n",
    "\n",
    "Una vez obtenido el vector de salida de la capade pooling, aplico una capa dropout para regularizar y, finalmente, una capa densa con activación sigmoide ya que se trata de un problema de clasificación multietiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, regularizers\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "\n",
    "# Entrada del modelo: los ids y la máscara de atención\n",
    "input_ids = Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Salida del modelo\n",
    "# outputs = transformer_model([input_ids, attention_mask])\n",
    "outputs = transformer_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    training=False\n",
    ")\n",
    "sequence_output = outputs.last_hidden_state \n",
    "\n",
    "# Capa de pooling\n",
    "POOLING_STRATEGY = \"cls\"  # puede ser cls o mean\n",
    "\n",
    "if POOLING_STRATEGY == \"cls\":\n",
    "    # Tomamos el embedding del primer token [CLS] de cada secuencia\n",
    "    pooled_output = sequence_output[:, 0, :] \n",
    "elif POOLING_STRATEGY == \"mean\":\n",
    "    # Calculamos la media de los embeddings reales (ignorando padding)\n",
    "    mask_expanded = tf.cast(tf.expand_dims(attention_mask, -1), tf.float32)  # shape [batch, seq_len, 1]\n",
    "    # Multiplicar cada token embedding por la máscara (0 para pads)\n",
    "    masked_output = sequence_output * mask_expanded\n",
    "    # Sumar en eje de secuencia y dividir por la suma de la máscara (número de tokens reales)\n",
    "    sum_embeddings = tf.reduce_sum(masked_output, axis=1)  # [batch, hidden_size]\n",
    "    num_tokens = tf.reduce_sum(mask_expanded, axis=1)      # [batch, 1]\n",
    "    pooled_output = sum_embeddings / (num_tokens + 1e-10)  # evitar la indeterminación\n",
    "\n",
    "# Aplicamos el dropout\n",
    "x = Dropout(DROPOUT_RATE)(pooled_output)\n",
    "\n",
    "# Aplicamos una capa densa para reducir la dimensionalidad y añadir no linealidad\n",
    "x = Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "\n",
    "# Aplicamos otro dropout para evitar el sobreajuste\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "# Capa densa de clasificación con activación sigmoide (multietiqueta)\n",
    "output_logits = Dense(NUM_LABELS, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "\n",
    "# Definimos el modelo completo\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output_logits)\n",
    "\n",
    "# Visualizamos el modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28084b9c",
   "metadata": {},
   "source": [
    "### 2.3 Compilación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af06281",
   "metadata": {},
   "source": [
    "Aquí añadí una línea para limpiar la caché de la sesión para ver si era eso lo que estaba causando el OOM, la he comentado porque el problema no venía de ahí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0eda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "\n",
    "# Limpiamos la sesión de Keras para evitar el OOM\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "# Compilamos el modelo con optimizador, función de pérdidas y métricas para la evaluación\n",
    "\"\"\"\n",
    "model.compile(optimizer=Adam(learning_rate=2e-5),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[AUC(name=\"AUC\", multi_label=True)])\n",
    "\"\"\"\n",
    "\n",
    "# Compilamos el modelo con AdamW, que es una variante de Adam con decaimiento de peso\n",
    "optimizer = AdamW(learning_rate=2e-5, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_ft = AdamW(learning_rate=2e-5, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5bc188",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcfd968",
   "metadata": {},
   "source": [
    "### 3.1 Guardado del modelo y callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a579966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Directorio para guardar el mejor modelo\n",
    "#checkpoint_path = \"best_model.h5\"\n",
    "clean_name = re.sub(r'[^A-Za-z0-9._-]', '_', MODEL_NAME)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_path = f\"{clean_name}_{timestamp}.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True, verbose=1), # Así evitamos el overfitting\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1), # Nos quedamos con el mejor modelo\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1), # Reducimos el learning rate si no mejora la val_loss\n",
    "    TensorBoard(log_dir=\"logs/fit\", histogram_freq=1),  # guarda logs para TensorBoard\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62ca98",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque de preentrenamiento: congelamos el modelo base y entrenamos solo la cabeza\n",
    "transformer_model.trainable = False # Congelamos el modelo base para el preentrenamiento\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=BinaryCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[AUC(name=\"AUC\", multi_label=True)]\n",
    ")\n",
    "history_head = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=UNFREEZE_EPOCH-1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque de fine-tuning: descongelamos el modelo base y entrenamos todo junto\n",
    "transformer_model.trainable = True # Descongelamos el modelo base para el fine-tuning\n",
    "# Hay que recompilar para que Keras vea los nuevos trainable flags\n",
    "model.compile(\n",
    "    optimizer=optimizer_ft,\n",
    "    loss=BinaryCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[AUC(name=\"AUC\", multi_label=True)]\n",
    ")\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    initial_epoch=UNFREEZE_EPOCH-1,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc1a9a",
   "metadata": {},
   "source": [
    "## 4. EVALUACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ebb40",
   "metadata": {},
   "source": [
    "### 4.1 Métricas de evaluación sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254f45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# Obtenemos las predicciones del modelo sobre el conjunto de test\n",
    "y_prob = model.predict(test_ds) \n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "y_true = y_test # Los valores reales de las etiquetas en formato multi-hot que hemos creado antes\n",
    "#print(\"Predicciones del modelo (probabilidades):\\n\", y_prob[:5])\n",
    "#print(\"Predicciones del modelo (clases):\\n\", y_pred[:5])\n",
    "#print(\"Etiquetas reales (multi-hot):\\n\", y_true[:5])\n",
    "\n",
    "# Calculamos las métricas globales\n",
    "precision_macro  = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "recall_macro     = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "f1_macro         = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall_weighted    = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1_weighted        = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "auc_macro        = roc_auc_score(y_true, y_prob, average='macro')\n",
    "auc_weighted     = roc_auc_score(y_true, y_prob, average='weighted')\n",
    "\n",
    "print(f\"Precisión macro:  {precision_macro:.4f}\")\n",
    "print(f\"Recall macro:      {recall_macro:.4f}\")\n",
    "print(f\"F1-score macro:    {f1_macro:.4f}\")\n",
    "print(f\"Precisión weighted: {precision_weighted:.4f}\")\n",
    "print(f\"Recall weighted:    {recall_weighted:.4f}\")\n",
    "print(f\"F1-score weighted:  {f1_weighted:.4f}\")\n",
    "print(f\"AUC macro:         {auc_macro:.4f}\")\n",
    "print(f\"AUC weighted:      {auc_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a2695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte de clasificación por etiqueta\n",
    "report = classification_report(y_true, y_pred, zero_division=0, output_dict=False)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acafb1a",
   "metadata": {},
   "source": [
    "### 4.2 Curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88472258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "# Extraemos el historial del entrenamiento y de validación\n",
    "epochs = range(1, len(history.history['loss'])+1)\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_auc = history.history.get('AUC')  \n",
    "val_auc = history.history.get('val_AUC')\n",
    "\n",
    "# Creamos la grafica de las pérdidas\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epochs, train_loss, 'o-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'o--', label='Validation Loss')\n",
    "plt.title('Curva de pérdida')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Crossentropy Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Creamos la grafica de AUC si está disponible\n",
    "if train_auc:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epochs, train_auc, 'o-', label='Training AUC')\n",
    "    plt.plot(epochs, val_auc, 'o--', label='Validation AUC')\n",
    "    plt.title('Curva de AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\n",
    "loss = history_head.history['loss'] + history_finetune.history['loss']\n",
    "val_loss = history_head.history['val_loss']  + history_finetune.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
    "plt.axvline(UNFREEZE_EPOCH-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "plt.title('Loss over epochs (head-only + fine-tuning)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc = history_head.history['AUC'] + history_finetune.history['AUC']\n",
    "val_auc = history_head.history['val_AUC']  + history_finetune.history['val_AUC']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, auc, 'bo-', label='Training AUC')\n",
    "plt.plot(epochs, val_auc, 'ro-', label='Validation AUC')\n",
    "plt.axvline(UNFREEZE_EPOCH-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "plt.title('AUC over epochs (head-only + fine-tuning)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a08d51",
   "metadata": {},
   "source": [
    "### 4.3 Matriz de confusión binaria por emoción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7217766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for label_idx in range(NUM_LABELS):\n",
    "    cm = confusion_matrix(y_true[:, label_idx], y_pred[:, label_idx], labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    label_name = str(label_idx)  \n",
    "    print(f\"Emoción {label_name}: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247d425",
   "metadata": {},
   "source": [
    "Aquí mapeo el id de las etiquetas al nombre real de la emoción. El problema es que no estoy seguro de si el mapeo es correcto, ya que cojo el nombre de las etiquetas del github de KOTE: https://github.com/searle-j/KOTE entonces no sé si el orden es el correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "LABELS = [\n",
    "    '불평/불만', '환영/호의', '감동/감탄', '지긋지긋', '고마움', '슬픔', '화남/분노', '존경',\n",
    "    '기대감', '우쭐댐/무시함', '안타까움/실망', '비장함', '의심/불신', '뿌듯함', '편안/쾌적',\n",
    "    '신기함/관심', '아껴주는', '부끄러움', '공포/무서움', '절망', '한심함', '역겨움/징그러움',\n",
    "    '짜증', '어이없음', '없음', '패배/자기혐오', '귀찮음', '힘듦/지침', '즐거움/신남', '깨달음',\n",
    "    '죄책감', '증오/혐오', '흐뭇함(귀여움/예쁨)', '당황/난처', '경악', '부담/안_내킴', '서러움',\n",
    "    '재미없음', '불쌍함/연민', '놀람', '행복', '불안/걱정', '기쁨', '안심/신뢰'\n",
    "]\n",
    "\n",
    "for label_idx, label_name in enumerate(LABELS):\n",
    "    cm = confusion_matrix(\n",
    "        y_true[:, label_idx],\n",
    "        y_pred[:, label_idx],\n",
    "        labels=[0, 1]\n",
    "    )\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"Emoción «{label_name}»: TP={tp}, FP={fp}, FN={fn}, TN={tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9bc8f",
   "metadata": {},
   "source": [
    "### 4.4 Predicciones fallidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificamos los índices de predicciones con errores\n",
    "error_indices = []\n",
    "for i in range(len(y_test)):\n",
    "    true_set = np.where(y_true[i] == 1)[0].tolist()\n",
    "    pred_set = np.where(y_pred[i] == 1)[0].tolist()\n",
    "    true_set = [str(x) for x in true_set]\n",
    "    pred_set = [str(x) for x in pred_set]\n",
    "    if true_set != pred_set: # Comparamos las etiquetas reales y predichas\n",
    "        error_indices.append((i, true_set, pred_set))\n",
    "\n",
    "print(f\"Número de ejemplos de prueba con error en al menos una etiqueta: {len(error_indices)}\")\n",
    "\n",
    "# Mostramos ejemplos al azar\n",
    "for idx, true_labels, pred_labels in random.sample(error_indices, 5):\n",
    "    text = test_texts[idx]\n",
    "    print(f\"\\nComentario: {text}\")\n",
    "    print(f\"Etiquetas reales: {', '.join(true_labels) if true_labels else 'None'}\")\n",
    "    print(f\"Etiquetas predichas: {', '.join(pred_labels) if pred_labels else 'None'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
